Epochs,Training Loss,Validation Loss
1,0.0,0.0
2,0.6630804077010193,1.0997732426303855
3,0.5079275198187996,0.5997732426303855
4,0.4297848244620612,0.7324263038548753
5,0.38590033975084936,0.5328798185941043
6,0.3754246885617214,0.26643990929705214
7,0.3774065685164213,0.46598639455782315
8,0.30889014722536806,0.0
9,0.32644394110985275,0.1326530612244898
10,0.2760475651189128,0.09977324263038549
11,0.3162514156285391,0.19954648526077098
12,0.28284258210645524,0.3333333333333333
13,0.24122310305775765,0.7664399092970522
14,0.23018120045300114,0.5328798185941043
15,0.21432616081540204,0.09977324263038549
16,0.232163080407701,0.5657596371882087
17,0.2287655719139298,0.06689342403628118
18,0.18091732729331822,0.3333333333333333
19,0.20526613816534542,0.19954648526077098
20,0.19365798414496035,0.4002267573696145
21,0.20186862967157418,0.16666666666666666
22,0.16562853907134767,0.1326530612244898
23,0.1664779161947905,0.06575963718820861
24,0.14949037372593432,0.3333333333333333
25,0.13901472253680633,0.032879818594104306
26,0.1432616081540204,0.0
27,0.14439411098527746,0.8662131519274376
28,0.1293884484711212,0.16666666666666666
29,0.11721404303510759,0.09977324263038549
30,0.1347678369195923,0.0
31,0.13448471121177802,0.16666666666666666
