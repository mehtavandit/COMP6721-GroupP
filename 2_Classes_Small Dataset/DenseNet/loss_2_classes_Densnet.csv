Epochs,Training Loss,Validation Loss
1,0.0,0.0
2,0.332955832389581,0.16666666666666666
3,0.2276330690826727,0.26643990929705214
4,0.20271800679501698,0.23356009070294784
5,0.1718573046432616,0.29931972789115646
6,0.14014722536806343,0.1326530612244898
7,0.1319365798414496,0.16666666666666666
8,0.12231030577576443,0.032879818594104306
9,0.12117780294450736,0.06575963718820861
10,0.10419026047565119,0.032879818594104306
11,0.10079275198187995,0.1326530612244898
12,0.0911664779161948,0.3333333333333333
13,0.08408833522083806,0.16666666666666666
14,0.07332955832389582,0.06575963718820861
15,0.06058890147225368,0.032879818594104306
16,0.07191392978482446,0.19954648526077098
17,0.052944507361268406,0.032879818594104306
18,0.045866364665911666,0.06575963718820861
19,0.057474518686296716,0.09977324263038549
20,0.050396375990939976,0.06689342403628118
21,0.028312570781426953,0.09977324263038549
22,0.03822197055492639,0.06689342403628118
23,0.028312570781426953,0.0
24,0.02406568516421291,0.09977324263038549
25,0.028312570781426953,0.0
26,0.028029445073612685,0.0
27,0.022083805209513023,0.19954648526077098
28,0.016138165345413364,0.032879818594104306
29,0.004530011325028313,0.0
30,0.010475651189127973,0.0
31,0.021234428086070217,0.06575963718820861
